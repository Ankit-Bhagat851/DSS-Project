{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment2: data cleaning\n",
    "# Given the information collected in the previous assignment, address the problem\n",
    "# related to the missing data (if any) and integrate the additional data (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacdcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdcdf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. CSV file saved as 'output.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from a file\n",
    "with open('tracks.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Ensure it's a list of records\n",
    "if isinstance(data, dict):\n",
    "    \n",
    "    data = [data]\n",
    "\n",
    "# Write to CSV\n",
    "with open('output.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"Conversion complete. CSV file saved as 'output.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c0f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML data has been successfully converted to output2.csv.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = ET.parse('artists.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract all rows\n",
    "rows = root.findall('row')\n",
    "\n",
    "# Get all unique tags from the first row (assuming all rows have same structure)\n",
    "headers = [elem.tag for elem in rows[0]]\n",
    "\n",
    "# Write to CSV\n",
    "with open('output2.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)  # write header\n",
    "\n",
    "    for row in rows:\n",
    "        writer.writerow([row.find(tag).text if row.find(tag) is not None else '' for tag in headers])\n",
    "\n",
    "print(\"XML data has been successfully converted to output2.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1734e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning completed. Data saved to: tracks.csv\n",
      " Rows removed with missing 'lyrics': 3\n"
     ]
    }
   ],
   "source": [
    "#Remove the row where 'lyrics' is null\n",
    "\n",
    "input_file = 'output.csv'\n",
    "output_file = 'tracks.csv'\n",
    "columnToCheck = 'lyrics'\n",
    "\n",
    "\n",
    "def cleanMissingData(input_file, output_file, columnToCheck):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, removes the rows with missing values \n",
    "    in the specified column, and saves the result to a new CSV.\n",
    "    \"\"\"\n",
    "    removed_rows = 0\n",
    "    missingID = ['NaN', 'nan', '']\n",
    "\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # DictReader to read the data as dictionary (easier to handle columns)\n",
    "        reader = csv.DictReader(infile)\n",
    "        campi = reader.fieldnames # Get the headers of the original file\n",
    "\n",
    "        # Open output file to write cleaned data\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=campi)\n",
    "            writer.writeheader() # Scrivi l'intestazione nel nuovo file\n",
    "\n",
    "            # Itera su ogni riga del file originale\n",
    "            for riga in reader:\n",
    "                lyricsValue = riga.get(columnToCheck, '') # Prendi il valore di 'lyrics'\n",
    "\n",
    "                # CLEANING STEP: check if the value is missing \n",
    "                # Applica .strip() per pulire eventuali spazi bianchi extra\n",
    "                if lyricsValue.strip() not in missingID:\n",
    "                    writer.writerow(riga)\n",
    "                else:\n",
    "                    removed_rows += 1\n",
    "\n",
    "    print(f\" Cleaning completed. Data saved to: {output_file}\")\n",
    "    print(f\" Rows removed with missing '{columnToCheck}': {removed_rows}\")\n",
    " \n",
    "\n",
    "# Esegui la funzione\n",
    "cleanMissingData(input_file, output_file, columnToCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd58c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data imputation completed.\n",
      " Total rows updated with calculated metrics: 73\n",
      " Cleaned and imputed data saved to: audioCleanData.csv\n"
     ]
    }
   ],
   "source": [
    "import re # Used for advanced splitting and cleaning\n",
    "# --- Configuration ---\n",
    "input_file1 = 'tracks.csv' # File from the previous cleaning step\n",
    "output_file1 = 'audioCleanData.csv'\n",
    "columnsToFill = ['n_sentences', 'n_tokens', 'char_per_tok', 'avg_token_per_clause']\n",
    "\n",
    "# --- Core Logic Functions ---\n",
    "\n",
    "def calculate_metrics(lyrics: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the linguistic metrics based on the song lyrics.\n",
    "    This function uses simple string manipulation as external libraries are forbidden.\n",
    "    \n",
    "    Args:\n",
    "        lyrics: The string containing the full song lyrics.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with the calculated values for the missing columns.\n",
    "    \"\"\"\n",
    "    if not lyrics or lyrics.strip() == '':\n",
    "        # Return zeros if lyrics are missing, though this should be rare for the 76 rows we target, we cleaned before.\n",
    "        return {\n",
    "            'n_sentences': 0, \n",
    "            'n_tokens': 0, \n",
    "            'char_per_tok': 0.0, \n",
    "            'avg_token_per_clause': 0.0\n",
    "        }\n",
    "    # 1. Tokenization (Counting Words/Tokens) --> to fill 'n_tokens'\n",
    "    # Use a regex to split the text by any sequence of non-alphanumeric characters (including spaces, punctuation, etc.)\n",
    "    # and filter out empty strings resulting from the split.\n",
    "    words = [token for token in re.split(r'[^a-zA-Z0-9]+', lyrics) if token]\n",
    "    n_tokens = len(words)\n",
    "\n",
    "    # 2. Character Count\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "\n",
    "    # 3. Sentence Count\n",
    "    # Assuming sentences are separated by newline characters (the simplest heuristic)\n",
    "    # This might overestimate or underestimate the true sentence count.\n",
    "    sentences = [s for s in lyrics.split('\\n') if s.strip() != '']\n",
    "    n_sentences = len(sentences)\n",
    "    \n",
    "    # 4. Calculate Derived Metrics\n",
    "    \n",
    "    # Average characters per token\n",
    "    char_per_tok = total_chars / n_tokens if n_tokens > 0 else 0.0\n",
    "    \n",
    "    # Average tokens per clause (using sentences as clauses)\n",
    "    avg_token_per_clause = n_tokens / n_sentences if n_sentences > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'n_sentences': n_sentences, \n",
    "        'n_tokens': n_tokens, \n",
    "        'char_per_tok': round(char_per_tok, 4), # Rounding for clean output\n",
    "        'avg_token_per_clause': round(avg_token_per_clause, 4)\n",
    "    }\n",
    "\n",
    "# --- Main Processing ---\n",
    "\n",
    "def fill_missing_metrics(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Main function to read the CSV, fill the missing metrics using lyrics, and write the new CSV.\n",
    "    \"\"\"\n",
    "    updated_rows_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames # Get the original header\n",
    "            \n",
    "            # Read all rows into memory for easier manipulation (assuming the file is not excessively large)\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found at {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Process Data\n",
    "    for row in data:\n",
    "        # Check if the row is one of the 76 missing ones (assuming missing values were left as empty strings)\n",
    "        # We also check if 'lyrics' is NOT missing, otherwise we cannot calculate\n",
    "        missing_values = ['', 'NaN', 'nan', None]\n",
    "        is_metrics_missing = any(row.get(col) in missing_values for col in columnsToFill)\n",
    "        has_lyrics = row.get('lyrics', '').strip() not in ['', 'NaN', 'nan']\n",
    "        \n",
    "        if is_metrics_missing and has_lyrics:\n",
    "            \n",
    "            # Calculate the new metrics using the available lyrics\n",
    "            new_metrics = calculate_metrics(row['lyrics'])\n",
    "            \n",
    "            # Update the row with the calculated values\n",
    "            for col, value in new_metrics.items():\n",
    "                row[col] = str(value) # CSV writers expect string values\n",
    "            \n",
    "            updated_rows_count += 1\n",
    "            \n",
    "        elif is_metrics_missing and not has_lyrics:\n",
    "             # In a production setting, you'd log or handle this case. \n",
    "             # Since we only removed 3 'lyrics' rows, this shouldn't affect the other 76 rows.\n",
    "             pass \n",
    "\n",
    "    # Write the updated data to the new file\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\" Data imputation completed.\")\n",
    "    print(f\" Total rows updated with calculated metrics: {updated_rows_count}\")\n",
    "    print(f\" Cleaned and imputed data saved to: {output_file}\")\n",
    "\n",
    "# Execute the main function\n",
    "fill_missing_metrics(input_file1, output_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb19d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Spotify API Imputation ---\n",
      "\n",
      " Spotify Imputation Complete.\n",
      " Total rows successfully imputed: 71\n",
      " Data saved to: DataPostSpotifyAPI.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import base64\n",
    "import time\n",
    "\n",
    "# --- Configuration and Constants ---\n",
    "\n",
    "INPUT_FILE = 'audioCleanData.csv' \n",
    "OUTPUT_FILE = 'DataPostSpotifyAPI.csv'\n",
    "MISSING_VALUES = ['', 'NaN', 'nan'] \n",
    "\n",
    "# !!! REPLACE THESE WITH YOUR ACTUAL SPOTIFY CREDENTIALS !!!\n",
    "SPOTIFY_CLIENT_ID = \"5838810d86504d6bbc6947b39b82d8f7\" \n",
    "SPOTIFY_CLIENT_SECRET = \"b467db2aef154993a7610f779203bc87\" \n",
    "\n",
    "# Spotify API Endpoints\n",
    "SPOTIFY_TOKEN_URL = \"https://accounts.spotify.com/api/token\"\n",
    "SPOTIFY_SEARCH_URL = \"https://api.spotify.com/v1/search\"\n",
    "SPOTIFY_FEATURES_URL = \"https://api.spotify.com/v1/audio-features\"\n",
    "\n",
    "# Columns to be Imputed\n",
    "AUDIO_COLUMNS = ['bpm', 'loudness', 'pitch', 'flux', 'rms', 'rolloff', 'flatness', 'spectral_complexity']\n",
    "ALBUM_COLUMNS = ['album_name', 'album_release_date', 'album_type', 'disc_number', 'track_number', 'duration_ms', 'explicit', 'popularity', 'id_album']\n",
    "LANGUAGE_COLUMN = 'language' \n",
    "ALL_EXTERNAL_COLUMNS = AUDIO_COLUMNS + ALBUM_COLUMNS + [LANGUAGE_COLUMN]\n",
    "\n",
    "# --- 1. Spotify Authentication ---\n",
    "\n",
    "def get_spotify_token(client_id: str, client_secret: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Obtains an Access Token from Spotify using the Client Credentials Flow.\n",
    "    This token is required for all subsequent API requests.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Base64 encode the Client ID and Secret for authentication header\n",
    "        auth_string = f\"{client_id}:{client_secret}\"\n",
    "        encoded_auth = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {encoded_auth}\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "        }\n",
    "        data = urllib.parse.urlencode({'grant_type': 'client_credentials'}).encode('utf-8')\n",
    "\n",
    "        req = urllib.request.Request(SPOTIFY_TOKEN_URL, data=data, headers=headers, method='POST')\n",
    "        \n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            token_info = json.loads(response.read().decode('utf-8'))\n",
    "            return token_info.get('access_token')\n",
    "            \n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"Error getting Spotify token: HTTP {e.code} - {e.reason}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Spotify token: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Spotify Data Fetching ---\n",
    "\n",
    "def fetch_spotify_data(title: str, artist: str, token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Searches Spotify for the track ID and then fetches its metadata and audio features.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing track metadata and audio features, or {} on failure.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    track_id = None\n",
    "    \n",
    "    # --- PHASE A: Search for Track ID ---\n",
    "    \n",
    "    try:\n",
    "        search_query = f\"track:{title} artist:{artist}\"\n",
    "        encoded_query = urllib.parse.quote(search_query)\n",
    "        search_url = f\"{SPOTIFY_SEARCH_URL}?q={encoded_query}&type=track&limit=1\"\n",
    "        \n",
    "        req = urllib.request.Request(search_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            search_results = json.loads(response.read().decode('utf-8'))\n",
    "        \n",
    "        items = search_results.get('tracks', {}).get('items', [])\n",
    "        \n",
    "        if not items:\n",
    "            return {} # Track not found\n",
    "        \n",
    "        # Extract ID and initial metadata\n",
    "        track_info = items[0]\n",
    "        track_id = track_info['id']\n",
    "        album_info = track_info.get('album', {})\n",
    "        \n",
    "        # Prepare initial data dictionary\n",
    "        data = {\n",
    "            'duration_ms': track_info.get('duration_ms'),\n",
    "            'explicit': str(track_info.get('explicit')),\n",
    "            'popularity': track_info.get('popularity'),\n",
    "            'album_name': album_info.get('name'),\n",
    "            'album_release_date': album_info.get('release_date'),\n",
    "            'album_type': album_info.get('album_type'),\n",
    "            'disc_number': track_info.get('disc_number'),\n",
    "            'track_number': track_info.get('track_number'),\n",
    "            'id_album': album_info.get('id'),\n",
    "            # Language is not reliably available via search endpoint, handle later\n",
    "        }\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Rate limit or other HTTP error\n",
    "        print(f\"DEBUG: Search failed for '{title}': HTTP {e.code}\")\n",
    "        if e.code == 429: # Too Many Requests\n",
    "            print(\"WARNING: Spotify Rate Limit hit. Pausing for 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        return {}\n",
    "    except Exception:\n",
    "        return {} # Parsing or network error\n",
    "\n",
    "    # --- PHASE B: Fetch Audio Features using Track ID ---\n",
    "\n",
    "    try:\n",
    "        features_url = f\"{SPOTIFY_FEATURES_URL}/{track_id}\"\n",
    "        req = urllib.request.Request(features_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            audio_features = json.loads(response.read().decode('utf-8'))\n",
    "        \n",
    "        # Impute audio features, mapping Spotify names to CSV column names\n",
    "        data['bpm'] = audio_features.get('tempo')\n",
    "        data['loudness'] = audio_features.get('loudness')\n",
    "        data['pitch'] = audio_features.get('key') # 'key' represents pitch class (0-11)\n",
    "        # Note: Spotify does not provide 'flux', 'rms', 'flatness', 'spectral_complexity', \n",
    "        # or 'rolloff' directly. These are usually derived from raw audio analysis.\n",
    "        # We will leave these as missing if Spotify doesn't have a direct equivalent.\n",
    "        \n",
    "        # --- Handle non-available columns (as required) ---\n",
    "        # If Spotify cannot provide these, they remain missing (NaN/empty string).\n",
    "        \n",
    "    except Exception:\n",
    "        # Audio feature fetching failed (e.g., track is missing features)\n",
    "        pass \n",
    "        \n",
    "    return data # Returns the gathered data (potentially incomplete)\n",
    "\n",
    "# --- 3. Main Imputation Logic ---\n",
    "\n",
    "def impute_external_data_spotify(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Implements the conservative imputation using the Spotify API.\n",
    "    Only fills the fields if data is successfully fetched; otherwise, preserves original value.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Spotify API Imputation ---\")\n",
    "    \n",
    "    # 1. Get Spotify Access Token\n",
    "    spotify_token = get_spotify_token(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)\n",
    "    if not spotify_token:\n",
    "        print(\"FATAL: Could not retrieve Spotify token. Cannot proceed with API calls.\")\n",
    "        return\n",
    "\n",
    "    imputed_count = 0\n",
    "    data_to_write = []\n",
    "\n",
    "    try:\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader) \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found at {input_file}.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate and Impute\n",
    "    for i, row in enumerate(data):\n",
    "        # Check if ANY external column is missing\n",
    "        is_missing_external = any(row.get(col, '').strip().lower() in MISSING_VALUES for col in ALL_EXTERNAL_COLUMNS)\n",
    "        \n",
    "        if is_missing_external and row.get('title') and row.get('primary_artist'):\n",
    "            \n",
    "            # Fetch data (will be {} if not found)\n",
    "            external_data = fetch_spotify_data(row['title'], row['primary_artist'], spotify_token)\n",
    "            updated = False\n",
    "            \n",
    "            # Impute the missing fields ONLY IF external_data has a specific value\n",
    "            for col in ALL_EXTERNAL_COLUMNS:\n",
    "                current_value = str(row.get(col, '')).strip().lower()\n",
    "                \n",
    "                # Check 1: Is the current value missing? \n",
    "                # Check 2: Did Spotify successfully return a non-None value for this column?\n",
    "                if current_value in MISSING_VALUES and col in external_data and external_data[col] is not None:\n",
    "                    row[col] = str(external_data[col]) # Update the row\n",
    "                    updated = True\n",
    "            \n",
    "            if updated:\n",
    "                imputed_count += 1\n",
    "        \n",
    "        data_to_write.append(row)\n",
    "        \n",
    "        # Add a delay to respect Spotify's rate limits\n",
    "        if i % 10 == 0 and i > 0:\n",
    "             time.sleep(0.1) \n",
    "\n",
    "    # 3. Write the updated data to the new file\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Spotify Imputation Complete.\")\n",
    "    print(f\" Total rows successfully imputed: {imputed_count}\")\n",
    "    print(f\" Data saved to: {output_file}\")\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "impute_external_data_spotify(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3680722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Date part imputation complete.\n",
      " Total rows where at least one date part was imputed: 1235\n",
      " File saved to: DataWithDateImputation.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'DataPostSpotifyAPI.csv' \n",
    "OUTPUT_FILE = 'DataWithDateImputation.csv'\n",
    "\n",
    "# Define all possible representations of missing data\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', None] \n",
    "\n",
    "# --- Main Imputation Function ---\n",
    "\n",
    "def impute_date_parts(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads the CSV and imputes missing values in existing 'year', 'month', \n",
    "    and 'day' columns using data from 'album_release_date'.\n",
    "    \"\"\"\n",
    "    data_to_write = []\n",
    "    imputed_rows = 0\n",
    "    \n",
    "    try:\n",
    "        # Read the input file and determine the fieldnames\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\" ERROR: Input file not found at: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Ensure the target columns exist in the file\n",
    "    if not all(col in fieldnames for col in ['year', 'month', 'day']):\n",
    "        print(\" ERROR: Required columns ('year', 'month', 'day') not found in the input file.\")\n",
    "        return\n",
    "        \n",
    "    for row in data:\n",
    "        release_date = row.get('album_release_date')\n",
    "        \n",
    "        # Check if the source date is available and valid\n",
    "        if release_date is None or str(release_date).strip().lower() in MISSING_VALUES:\n",
    "            # If the source date is missing, we cannot impute, so we skip\n",
    "            data_to_write.append(row)\n",
    "            continue\n",
    "            \n",
    "        date_updated = False\n",
    "        \n",
    "        # Determine which columns are currently missing\n",
    "        is_year_missing = str(row.get('year', '')).strip().lower() in MISSING_VALUES\n",
    "        is_month_missing = str(row.get('month', '')).strip().lower() in MISSING_VALUES\n",
    "        is_day_missing = str(row.get('day', '')).strip().lower() in MISSING_VALUES\n",
    "        \n",
    "        # Only proceed if at least one target column is missing\n",
    "        if is_year_missing or is_month_missing or is_day_missing:\n",
    "            \n",
    "            # --- Parsing Logic ---\n",
    "            try:\n",
    "                # 1. Try YYYY-MM-DD format (Full date)\n",
    "                dt_obj = datetime.strptime(release_date, '%Y-%m-%d')\n",
    "                \n",
    "                if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                if is_month_missing: row['month'] = dt_obj.month; date_updated = True\n",
    "                if is_day_missing: row['day'] = dt_obj.day; date_updated = True\n",
    "                \n",
    "            except ValueError:\n",
    "                # 2. Try YYYY-MM format (Month granularity)\n",
    "                try:\n",
    "                    dt_obj = datetime.strptime(release_date, '%Y-%m')\n",
    "                    \n",
    "                    if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                    if is_month_missing: row['month'] = dt_obj.month; date_updated = True\n",
    "                    # Day remains missing if it was missing initially\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # 3. Try YYYY format (Year granularity only)\n",
    "                    try:\n",
    "                        dt_obj = datetime.strptime(release_date, '%Y')\n",
    "                        \n",
    "                        if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                        # Month and Day remain missing if they were missing initially\n",
    "                        \n",
    "                    except ValueError:\n",
    "                        # Parsing failed completely or the date is malformed\n",
    "                        pass \n",
    "        \n",
    "        if date_updated:\n",
    "            imputed_rows += 1\n",
    "            \n",
    "        data_to_write.append(row)\n",
    "\n",
    "    # --- Write Output ---\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Date part imputation complete.\")\n",
    "    print(f\" Total rows where at least one date part was imputed: {imputed_rows}\")\n",
    "    print(f\" File saved to: {output_file}\")\n",
    "\n",
    "# Execute the function\n",
    "impute_date_parts(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3330f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning completed. Data saved to: tracks_albumClean.csv\n",
      " Rows removed with missing 'id_album': 7\n"
     ]
    }
   ],
   "source": [
    "#Remove the row where 'album_id' is null\n",
    "\n",
    "input_file = 'DataWithDateImputation.csv'\n",
    "output_file = 'tracks_albumClean.csv'\n",
    "columnToCheck = 'id_album'\n",
    "\n",
    "\n",
    "def cleanMissingData(input_file, output_file, columnToCheck):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, removes the rows with missing values \n",
    "    in the specified column, and saves the result to a new CSV.\n",
    "    \"\"\"\n",
    "    removed_rows = 0\n",
    "    missingID = ['NaN', 'nan', '']\n",
    "\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # DictReader to read the data as dictionary (easier to handle columns)\n",
    "        reader = csv.DictReader(infile)\n",
    "        campi = reader.fieldnames # Get the headers of the original file\n",
    "\n",
    "        # Open output file to write cleaned data\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=campi)\n",
    "            writer.writeheader() # Scrivi l'intestazione nel nuovo file\n",
    "\n",
    "            # Itera su ogni riga del file originale\n",
    "            for riga in reader:\n",
    "                lyricsValue = riga.get(columnToCheck, '') # Prendi il valore di 'lyrics'\n",
    "\n",
    "                # CLEANING STEP: check if the value is missing \n",
    "                # Applica .strip() per pulire eventuali spazi bianchi extra\n",
    "                if lyricsValue.strip() not in missingID:\n",
    "                    writer.writerow(riga)\n",
    "                else:\n",
    "                    removed_rows += 1\n",
    "\n",
    "    print(f\" Cleaning completed. Data saved to: {output_file}\")\n",
    "    print(f\" Rows removed with missing '{columnToCheck}': {removed_rows}\")\n",
    " \n",
    "cleanMissingData(input_file, output_file, columnToCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e265cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Imputation 'featured_artists' completed.\n",
      " Total number updated rows: 7640\n",
      " File saved as: TracksWithNoFeature.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_FILE = 'tracks_albumClean.csv' \n",
    "OUTPUT_FILE = 'TracksWithNoFeature.csv'\n",
    "\n",
    "# Define all values that should be treated as \"missing\"\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', None] \n",
    "\n",
    "# --- Imputation Function ---\n",
    "\n",
    "def impute_featured_artists(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and replaces missing values in 'featured_artists' with 'no Feature'.\n",
    "    \"\"\"\n",
    "    data_to_write = []\n",
    "    imputation_count = 0\n",
    "    \n",
    "    try:\n",
    "        # 1. Read Data\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: File not found: {input_file}\")\n",
    "        return\n",
    "\n",
    "    if 'featured_artists' not in fieldnames:\n",
    "        print(\" Error: the column 'featured_artists' was not found.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Process Data (Imputation)\n",
    "    for row in data:\n",
    "        # Get the current value, ensuring it's treated as a string\n",
    "        current_value = str(row.get('featured_artists', '')).strip()\n",
    "        \n",
    "        # Check if the value is missing\n",
    "        if current_value.lower() in MISSING_VALUES:\n",
    "            # Impute the missing value\n",
    "            row['featured_artists'] = 'No Feature'\n",
    "            imputation_count += 1\n",
    "            \n",
    "        data_to_write.append(row)\n",
    "\n",
    "    # 3. Write Output\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Imputation 'featured_artists' completed.\")\n",
    "    print(f\" Total number updated rows: {imputation_count}\")\n",
    "    print(f\" File saved as: {output_file}\")\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "impute_featured_artists(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f147b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f4c5a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Imputation 'album' completed.\n",
      " Total number updated rows: 1510\n",
      " File saved as: TracksAlbumNameImputed.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_FILE = 'TracksWithNoFeature.csv' # Assumiamo sia il file pulito dall'ultima operazione\n",
    "OUTPUT_FILE = 'TracksAlbumNameImputed.csv'\n",
    "\n",
    "# Define all values that should be treated as \"missing\" for the 'album' column\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', None] \n",
    "\n",
    "# --- Imputation Function ---\n",
    "\n",
    "def impute_album_from_clean_column(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and fills missing values in the 'album' column \n",
    "    using the clean 'album name' column in the same row.\n",
    "    \"\"\"\n",
    "    data_to_write = []\n",
    "    imputation_count = 0\n",
    "    \n",
    "    try:\n",
    "        # 1. Read Data\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: File was not found at: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Check for both columns\n",
    "    if 'album' not in fieldnames or 'album_name' not in fieldnames:\n",
    "        print(\" Error: the columns 'album' e/o 'album name' were not found.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Process Data (Conditional Imputation)\n",
    "    for row in data:\n",
    "        # Get the current value of 'album'\n",
    "        current_album = str(row.get('album', '')).strip().lower()\n",
    "        \n",
    "        # Get the value from the clean column\n",
    "        clean_album_name = row.get('album_name')\n",
    "        \n",
    "        # Imputation logic: If 'album' is missing AND 'album name' is present and clean\n",
    "        if current_album in MISSING_VALUES and clean_album_name is not None and str(clean_album_name).strip() != '':\n",
    "            \n",
    "            # Copy the value from the clean column to the missing column\n",
    "            row['album'] = clean_album_name\n",
    "            imputation_count += 1\n",
    "            \n",
    "        data_to_write.append(row)\n",
    "\n",
    "    # 3. Write Output\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Imputation 'album' completed.\")\n",
    "    print(f\" Total number updated rows: {imputation_count}\")\n",
    "    print(f\" File saved as: {output_file}\")\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "impute_album_from_clean_column(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faabe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Column 'loudness' removed successfully.\n",
      " Total number of row: 11156\n",
      " File saved as: TracksWithoutLoudness.csv\n"
     ]
    }
   ],
   "source": [
    "#Remove the column 'loudness' because it has perfect correlation with rms\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'TracksAlbumNameImputed.csv' \n",
    "OUTPUT_FILE = 'TracksWithoutLoudness.csv'\n",
    "\n",
    "# Column to be removed\n",
    "COLUMN_TO_REMOVE = 'loudness' \n",
    "\n",
    "# --- Main Removal Function ---\n",
    "\n",
    "def remove_column_from_csv(input_file: str, output_file: str, column_to_remove: str):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, removes the specified column, and writes the result \n",
    "    to a new CSV file using Python's standard 'csv' module.\n",
    "    \"\"\"\n",
    "    data_to_write = []\n",
    "    \n",
    "    try:\n",
    "        # 1. Read the input file and determine fieldnames\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            # Get the original list of fieldnames (columns)\n",
    "            original_fieldnames = reader.fieldnames\n",
    "            \n",
    "            # Create the new list of fieldnames, excluding the column to be removed\n",
    "            if column_to_remove in original_fieldnames:\n",
    "                new_fieldnames = [name for name in original_fieldnames if name != column_to_remove]\n",
    "            else:\n",
    "                print(f\" Error: the column '{column_to_remove}' was not found.\")\n",
    "                return\n",
    "            \n",
    "            # 2. Process Data\n",
    "            for row in reader:\n",
    "                # Remove the key-value pair corresponding to the column\n",
    "                if column_to_remove in row:\n",
    "                    del row[column_to_remove]\n",
    "                \n",
    "                data_to_write.append(row)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: File not found: {input_file}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\" ERRORE during the reading: {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Write Output\n",
    "    try:\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=new_fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data_to_write)\n",
    "\n",
    "        print(f\"\\n Column '{column_to_remove}' removed successfully.\")\n",
    "        print(f\" Total number of row: {len(data_to_write)}\")\n",
    "        print(f\" File saved as: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE during the writing: {e}\")\n",
    "\n",
    "\n",
    "# --- Execution ---\n",
    "remove_column_from_csv(INPUT_FILE, OUTPUT_FILE, COLUMN_TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75eefc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data loaded successfully. Total rows: 11156\n",
      "ðŸ’¾ Plot saved successfully to Audio_Features_Distribution.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'TracksWithoutLoudness.csv'\n",
    "OUTPUT_IMAGE = 'Audio_Features_Distribution.png'\n",
    "\n",
    "# Columns containing the audio features\n",
    "FEATURE_COLUMNS = [\n",
    "    'bpm', \n",
    "    'rolloff', \n",
    "    'flux', \n",
    "    'rms', \n",
    "    'flatness', \n",
    "    'spectral_complexity', \n",
    "    'pitch'\n",
    "]\n",
    "\n",
    "def plot_audio_features_distribution(input_file: str, output_image: str, columns: list):\n",
    "    \"\"\"\n",
    "    Loads data using Pandas, creates histograms for the specified audio features, \n",
    "    and saves the resulting plot.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\" Data loaded successfully. Total rows: {len(df)}\")\n",
    "        \n",
    "        # Ensure all feature columns exist and convert them to numeric (errors='coerce' \n",
    "        # handles any remaining non-numeric data by setting it to NaN, which is safe for plotting)\n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            else:\n",
    "                print(f\" Column '{col}' not found. Skipping.\")\n",
    "        \n",
    "        # Select the existing and numeric columns for plotting\n",
    "        df_features = df[[col for col in columns if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]]\n",
    "\n",
    "        if df_features.empty:\n",
    "            print(\" ERROR: No numeric feature columns found to plot.\")\n",
    "            return\n",
    "\n",
    "        # --- Plotting ---\n",
    "        \n",
    "        # Determine grid size (e.g., 3 columns, auto rows)\n",
    "        n_cols = 3\n",
    "        n_rows = (len(df_features.columns) + n_cols - 1) // n_cols\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 5 * n_rows))\n",
    "        axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "        for i, col in enumerate(df_features.columns):\n",
    "            # Plot histogram for each feature\n",
    "            df_features[col].hist(ax=axes[i], bins=30, edgecolor='black', color='#1DB954') \n",
    "            axes[i].set_title(f'Distribution of {col}', fontsize=14)\n",
    "            axes[i].set_xlabel(col, fontsize=12)\n",
    "            axes[i].set_ylabel('Frequency', fontsize=12)\n",
    "            axes[i].grid(axis='y', alpha=0.5)\n",
    "\n",
    "        # Hide unused subplots\n",
    "        for j in range(len(df_features.columns), len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(\"Distribution of Audio Features\", fontsize=16, y=1.02)\n",
    "        \n",
    "        # Save the plot\n",
    "        plt.savefig(output_image)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"ðŸ’¾ Plot saved successfully to {output_image}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" ERROR: Input file '{input_file}' not found. Please check the file name.\")\n",
    "    except Exception as e:\n",
    "        print(f\" An error occurred during processing: {e}\")\n",
    "\n",
    "# --- Execution ---\n",
    "plot_audio_features_distribution(INPUT_FILE, OUTPUT_IMAGE, FEATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cccb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpm: distribution around 80 and 140 --> computation with median because of outliers.\n",
    "# rolloff: asimmetric distribution with long right tail --> computation with median because of skewness.\n",
    "# flux:  relatively symmetrical distribution --> mean\n",
    "# rms: relatively symmetrical distribution --> mean or median\n",
    "# flatness: bimodal/left-skewed distribution --> mode\n",
    "# spectral_complexity: normale --> mean\n",
    "# pitch: normal --> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "919419aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Calculated Imputation Values: {'flux': 1.258928489001082, 'rms': 0.22403263613415073, 'spectral_complexity': 27.419731644428417, 'pitch': 2255.962338216733, 'bpm': 106.975, 'rolloff': 1551.32825, 'flatness': 0.88245}\n",
      "\n",
      "âœ… Hybrid audio features imputation complete.\n",
      "ðŸ“Š Total rows with imputed values: 64\n",
      "ðŸ’¾ File saved to: TracksAlmostClean.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'TracksWithoutLoudness.csv' \n",
    "OUTPUT_FILE = 'TracksAlmostClean.csv'\n",
    "\n",
    "# Strategy Definition\n",
    "MEDIAN_FEATURES = ['bpm', 'rolloff']\n",
    "MEAN_FEATURES = ['flux', 'rms', 'spectral_complexity', 'pitch']\n",
    "MODE_FEATURES = ['flatness'] # Will use a simple mode imputation if possible, otherwise Median\n",
    "\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', None, '0'] \n",
    "\n",
    "# --- I. Calculation Functions ---\n",
    "\n",
    "def calculate_mean(data_list: list) -> float:\n",
    "    \"\"\"Calculates the mean of a list of numbers.\"\"\"\n",
    "    return sum(data_list) / len(data_list) if data_list else 0.0\n",
    "\n",
    "def calculate_median(data_list: list) -> float:\n",
    "    \"\"\"Calculates the median of a list of numbers.\"\"\"\n",
    "    if not data_list:\n",
    "        return 0.0\n",
    "    data_list.sort()\n",
    "    n = len(data_list)\n",
    "    if n % 2 == 0:\n",
    "        # Even number of elements, average the two middle values\n",
    "        return (data_list[n // 2 - 1] + data_list[n // 2]) / 2\n",
    "    else:\n",
    "        # Odd number of elements, return the middle element\n",
    "        return data_list[n // 2]\n",
    "\n",
    "def calculate_imputation_values(data: list[dict]) -> dict:\n",
    "    \"\"\"Calculates Mean/Median/Mode for all feature columns based on strategy.\"\"\"\n",
    "    \n",
    "    # 1. Gather numerical data for all features\n",
    "    feature_data = {col: [] for col in MEDIAN_FEATURES + MEAN_FEATURES + MODE_FEATURES}\n",
    "    \n",
    "    for row in data:\n",
    "        for col in feature_data.keys():\n",
    "            value_str = str(row.get(col, '')).strip()\n",
    "            if value_str.lower() not in MISSING_VALUES and value_str.replace('.', '', 1).isdigit():\n",
    "                try:\n",
    "                    feature_data[col].append(float(value_str))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    # 2. Calculate Imputation Values\n",
    "    imputation_values = {}\n",
    "    \n",
    "    for col in MEAN_FEATURES:\n",
    "        imputation_values[col] = calculate_mean(feature_data[col])\n",
    "        \n",
    "    for col in MEDIAN_FEATURES:\n",
    "        imputation_values[col] = calculate_median(feature_data[col])\n",
    "\n",
    "    # 3. Simple Mode for 'flatness' (approximating the largest peak 0.8-0.9)\n",
    "    # Due to complexity of calculating true mode without libraries, we approximate the mode:\n",
    "    # We will simply use the Median as a robust substitute if a simple Mode calculation is too complex.\n",
    "    imputation_values['flatness'] = calculate_median(feature_data['flatness']) \n",
    "    \n",
    "    return imputation_values\n",
    "\n",
    "# --- II. Main Imputation Logic ---\n",
    "\n",
    "def impute_audio_features_hybrid(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Implements the Hybrid imputation strategy: Median for skewed, Mean for normal.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Read Data and Calculate Imputation Values\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader)\n",
    "            \n",
    "        impute_map = calculate_imputation_values(data)\n",
    "        print(f\" Calculated Imputation Values: {impute_map}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" ERROR: File not found at: {input_file}\")\n",
    "        return\n",
    "        \n",
    "    imputation_count = 0\n",
    "    data_to_write = []\n",
    "\n",
    "    # 2. Impute Missing Values\n",
    "    all_features = MEDIAN_FEATURES + MEAN_FEATURES + MODE_FEATURES\n",
    "    \n",
    "    for row in data:\n",
    "        updated = False\n",
    "        for col in all_features:\n",
    "            current_value = str(row.get(col, '')).strip().lower()\n",
    "            \n",
    "            if current_value in MISSING_VALUES or current_value == '0':\n",
    "                \n",
    "                impute_val = impute_map.get(col, 0.0)\n",
    "                \n",
    "                if impute_val != 0.0:\n",
    "                    row[col] = round(impute_val, 4) # Impute the calculated value\n",
    "                    updated = True\n",
    "                \n",
    "        if updated:\n",
    "            imputation_count += 1\n",
    "            \n",
    "        data_to_write.append(row)\n",
    "\n",
    "    # 3. Write Output\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\nâœ… Hybrid audio features imputation complete.\")\n",
    "    print(f\"ðŸ“Š Total rows with imputed values: {imputation_count}\")\n",
    "    print(f\"ðŸ’¾ File saved to: {output_file}\")\n",
    "\n",
    "# --- Execution ---\n",
    "impute_audio_features_hybrid(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0957d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #JUST TO CHECK--> CANNOT USE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# dff = pd.read_csv('TracksAlmostClean.csv')\n",
    "\n",
    "# missing_countsPostSpotify = dff.isnull().sum() \n",
    "# # print(\"The number of missing values is: \\n\",missing_counts[missing_counts > 0]) #28 columns out of 37\n",
    "# missing_countsPostSpotify = missing_countsPostSpotify[missing_countsPostSpotify > 0].to_frame(name=\"missing_values\")\n",
    "# print(missing_countsPostSpotify)\n",
    "\n",
    "\n",
    "# pattern = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "# invalid_format = dff[~dff['album_release_date'].str.match(pattern, na=False)]\n",
    "# invalid_format\n",
    "\n",
    "# dff['album_release_date'].str.len().value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# dff['album_release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#           missing_values\n",
    "# language             104\n",
    "# month                107\n",
    "# day                  130\n",
    "\n",
    "#dont know but in album_release_date we have 261 with length 4 (so just the year) and the other with length 10 YYYY-MM-DD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d879447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################Ã Ã "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dab797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV_FILE = 'output2.csv'\n",
    "OUTPUT_FILE = 'artistClean.csv'\n",
    "USER_AGENT = \"MyApp/1.0 (email@example.com)\" # Required by APIs for identification\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', '0', 0] # Define all values that count as missing/zero\n",
    "\n",
    "# --- MusicBrainz Endpoints ---\n",
    "MB_URL = \"http://musicbrainz.org/ws/2/artist/\"\n",
    "NOMINATIM_URL = \"http://nominatim.openstreetmap.org/search\" # OpenStreetMap Geocoding\n",
    "\n",
    "# --- Utility Functions (APIs using urllib) ---\n",
    "\n",
    "def safe_http_get(url: str, params: dict, headers: dict) -> dict | None:\n",
    "    \"\"\"Performs a GET request using urllib and returns JSON data.\"\"\"\n",
    "    \n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    full_url = f\"{url}?{query_string}\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() == 200:\n",
    "                data = response.read().decode('utf-8')\n",
    "                return json.loads(data)\n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.getcode()} for URL: {url}\")\n",
    "                return None\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Catch specific HTTP errors\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch network or parsing errors\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    data = safe_http_get(MB_URL, params, headers)\n",
    "    time.sleep(1) # Respect API rate limits (1 request/second)\n",
    "    \n",
    "    if data is None or \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        # MusicBrainz usually provides the start date of the lifespan\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"), \n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # Extract country from the 'area' object\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "        \n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract country (Simplified)\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country using heuristics.\n",
    "    Returns a dictionary with available country data.\n",
    "    \"\"\"\n",
    "    wiki_url = \"https://en.wikipedia.org/api/rest_v1/page/summary/\"\n",
    "    search_path = urllib.parse.quote(name.replace(' ', '_'))\n",
    "    full_url = f\"{wiki_url}{search_path}\"\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    # Since Wikipedia API endpoint does not take params in the same way, we request directly\n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() != 200: return None\n",
    "            data = json.loads(response.read().decode('utf-8'))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(1) # Respect rate limits\n",
    "    \n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description (as per original logic)\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place (OpenStreetMap Nominatim)\n",
    "# ----------------------------\n",
    "def geocode_place(place_name: str, country_name: str | None = None) -> tuple[float or None, float or None]:\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "        \n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    # Nominatim requires a distinct User-Agent and is sensitive to rate limits\n",
    "    headers = {\"User-Agent\": \"MyDataPipeline/1.0 (contact@example.com)\"} \n",
    "\n",
    "    # Geocoding logic using safe_http_get structure\n",
    "    data = safe_http_get(NOMINATIM_URL, params, headers)\n",
    "    time.sleep(1) # Respect Nominatim usage policy (1 request/second)\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "    except (ValueError, KeyError):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV_FILE = 'output2.csv'\n",
    "OUTPUT_FILE = 'artistClean.csv'\n",
    "USER_AGENT = \"MyApp/1.0 (email@example.com)\" # Required by APIs for identification\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', '0', 0] # Define all values that count as missing/zero\n",
    "\n",
    "# --- MusicBrainz Endpoints ---\n",
    "MB_URL = \"http://musicbrainz.org/ws/2/artist/\"\n",
    "NOMINATIM_URL = \"http://nominatim.openstreetmap.org/search\" # OpenStreetMap Geocoding\n",
    "\n",
    "# --- Utility Functions (APIs using urllib) ---\n",
    "\n",
    "def safe_http_get(url: str, params: dict, headers: dict) -> dict | None:\n",
    "    \"\"\"Performs a GET request using urllib and returns JSON data.\"\"\"\n",
    "    \n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    full_url = f\"{url}?{query_string}\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() == 200:\n",
    "                data = response.read().decode('utf-8')\n",
    "                return json.loads(data)\n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.getcode()} for URL: {url}\")\n",
    "                return None\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Catch specific HTTP errors\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch network or parsing errors\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    data = safe_http_get(MB_URL, params, headers)\n",
    "    time.sleep(1) # Respect API rate limits (1 request/second)\n",
    "    \n",
    "    if data is None or \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        # MusicBrainz usually provides the start date of the lifespan\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"), \n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # Extract country from the 'area' object\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "        \n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract country (Simplified)\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country using heuristics.\n",
    "    Returns a dictionary with available country data.\n",
    "    \"\"\"\n",
    "    wiki_url = \"https://en.wikipedia.org/api/rest_v1/page/summary/\"\n",
    "    search_path = urllib.parse.quote(name.replace(' ', '_'))\n",
    "    full_url = f\"{wiki_url}{search_path}\"\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    # Since Wikipedia API endpoint does not take params in the same way, we request directly\n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() != 200: return None\n",
    "            data = json.loads(response.read().decode('utf-8'))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(1) # Respect rate limits\n",
    "    \n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description (as per original logic)\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place (OpenStreetMap Nominatim)\n",
    "# ----------------------------\n",
    "def geocode_place(place_name: str, country_name: str | None = None) -> tuple[float or None, float or None]:\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "        \n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    # Nominatim requires a distinct User-Agent and is sensitive to rate limits\n",
    "    headers = {\"User-Agent\": \"MyDataPipeline/1.0 (contact@example.com)\"} \n",
    "\n",
    "    # Geocoding logic using safe_http_get structure\n",
    "    data = safe_http_get(NOMINATIM_URL, params, headers)\n",
    "    time.sleep(1) # Respect Nominatim usage policy (1 request/second)\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "    except (ValueError, KeyError):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7451e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name):\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data: birth_place, country, birth_date, active_start, active_end\n",
    "    \"\"\"\n",
    "    url = \"https://musicbrainz.org/ws/2/artist/\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"MyApp/1.0 (email@example.com)\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    data = r.json()\n",
    "    if \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # MusicBrainz provides the artist's area (country)\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract city/country\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name):\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}\"\n",
    "    headers = {\"User-Agent\": \"MyApp/1.0 (email@example.com)\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    data = r.json()\n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        # add more cases if needed\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place\n",
    "# ----------------------------\n",
    "def geocode_place(place_name, country_name=None):\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None, None\n",
    "    \n",
    "    data = r.json()\n",
    "    if len(data) == 0:\n",
    "        return None, None\n",
    "    return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Update artists without latitude/longitude\n",
    "# ----------------------------\n",
    "for artist in df_artists:\n",
    "    if artist[\"latitude\"] == 0 or artist[\"longitude\"] == 0:\n",
    "        print(f\"Processing: {artist['name']}\")\n",
    "        \n",
    "        # 1ï¸âƒ£ Try MusicBrainz first\n",
    "        info = search_artist_musicbrainz(artist[\"name\"])\n",
    "        time.sleep(1)\n",
    "        if info:\n",
    "            artist[\"birth_place\"] = info.get(\"birth_place\") or artist.get(\"birth_place\")\n",
    "            artist[\"country\"] = info.get(\"country\") or artist.get(\"country\")\n",
    "            artist[\"birth_date\"] = info.get(\"birth_date\") or artist.get(\"birth_date\")\n",
    "            artist[\"active_start\"] = info.get(\"active_start\") or artist.get(\"active_start\")\n",
    "            artist[\"active_end\"] = info.get(\"active_end\") or artist.get(\"active_end\")\n",
    "        \n",
    "        # 2ï¸âƒ£ If country still missing, try Wikipedia\n",
    "        if not artist.get(\"country\"):\n",
    "            wiki_info = search_artist_wikipedia(artist[\"name\"])\n",
    "            time.sleep(1)\n",
    "            if wiki_info and wiki_info.get(\"country\"):\n",
    "                artist[\"country\"] = wiki_info[\"country\"]\n",
    "        \n",
    "        # 3ï¸âƒ£ Geocode if we have at least birth_place or country\n",
    "        place = artist.get(\"birth_place\") or artist.get(\"country\")\n",
    "        country = artist.get(\"country\") if artist.get(\"birth_place\") else None\n",
    "        if place:\n",
    "            lat, lon = geocode_place(place, country)\n",
    "            if lat is not None and lon is not None:\n",
    "                artist[\"latitude\"] = lat\n",
    "                artist[\"longitude\"] = lon\n",
    "            time.sleep(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Final check of artists data\n",
    "# ----------------------------\n",
    "for artist in df_artists:\n",
    "    print(artist[\"name\"], artist[\"birth_place\"], artist[\"country\"], artist[\"latitude\"], artist[\"longitude\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
