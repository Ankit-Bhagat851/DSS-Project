{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment2: data cleaning\n",
    "# Given the information collected in the previous assignment, address the problem\n",
    "# related to the missing data (if any) and integrate the additional data (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacdcda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdcdf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. CSV file saved as 'output.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load JSON data from a file\n",
    "with open('tracks.json', 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Ensure it's a list of records\n",
    "if isinstance(data, dict):\n",
    "    \n",
    "    data = [data]\n",
    "\n",
    "# Write to CSV\n",
    "with open('output.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(\"Conversion complete. CSV file saved as 'output.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c0f17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML data has been successfully converted to output2.csv.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = ET.parse('artists.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract all rows\n",
    "rows = root.findall('row')\n",
    "\n",
    "# Get all unique tags from the first row (assuming all rows have same structure)\n",
    "headers = [elem.tag for elem in rows[0]]\n",
    "\n",
    "# Write to CSV\n",
    "with open('output2.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)  # write header\n",
    "\n",
    "    for row in rows:\n",
    "        writer.writerow([row.find(tag).text if row.find(tag) is not None else '' for tag in headers])\n",
    "\n",
    "print(\"XML data has been successfully converted to output2.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1734e168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning completed. Data saved to: tracks.csv\n",
      " Rows removed with missing 'lyrics': 3\n"
     ]
    }
   ],
   "source": [
    "#Remove the row where 'lyrics' is null\n",
    "\n",
    "input_file = 'output.csv'\n",
    "output_file = 'tracks.csv'\n",
    "columnToCheck = 'lyrics'\n",
    "\n",
    "\n",
    "def cleanMissingData(input_file, output_file, columnToCheck):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, removes the rows with missing values \n",
    "    in the specified column, and saves the result to a new CSV.\n",
    "    \"\"\"\n",
    "    removed_rows = 0\n",
    "    missingID = ['NaN', 'nan', '']\n",
    "\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        # DictReader to read the data as dictionary (easier to handle columns)\n",
    "        reader = csv.DictReader(infile)\n",
    "        campi = reader.fieldnames # Get the headers of the original file\n",
    "\n",
    "        # Open output file to write cleaned data\n",
    "        with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=campi)\n",
    "            writer.writeheader() # Scrivi l'intestazione nel nuovo file\n",
    "\n",
    "            # Itera su ogni riga del file originale\n",
    "            for riga in reader:\n",
    "                lyricsValue = riga.get(columnToCheck, '') # Prendi il valore di 'lyrics'\n",
    "\n",
    "                # CLEANING STEP: check if the value is missing \n",
    "                # Applica .strip() per pulire eventuali spazi bianchi extra\n",
    "                if lyricsValue.strip() not in missingID:\n",
    "                    writer.writerow(riga)\n",
    "                else:\n",
    "                    removed_rows += 1\n",
    "\n",
    "    print(f\" Cleaning completed. Data saved to: {output_file}\")\n",
    "    print(f\" Rows removed with missing '{columnToCheck}': {removed_rows}\")\n",
    " \n",
    "\n",
    "# Esegui la funzione\n",
    "cleanMissingData(input_file, output_file, columnToCheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd58c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data imputation completed.\n",
      " Total rows updated with calculated metrics: 73\n",
      " Cleaned and imputed data saved to: audioCleanData.csv\n"
     ]
    }
   ],
   "source": [
    "import re # Used for advanced splitting and cleaning\n",
    "# --- Configuration ---\n",
    "input_file1 = 'tracks.csv' # File from the previous cleaning step\n",
    "output_file1 = 'audioCleanData.csv'\n",
    "columnsToFill = ['n_sentences', 'n_tokens', 'char_per_tok', 'avg_token_per_clause']\n",
    "\n",
    "# --- Core Logic Functions ---\n",
    "\n",
    "def calculate_metrics(lyrics: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates the linguistic metrics based on the song lyrics.\n",
    "    This function uses simple string manipulation as external libraries are forbidden.\n",
    "    \n",
    "    Args:\n",
    "        lyrics: The string containing the full song lyrics.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with the calculated values for the missing columns.\n",
    "    \"\"\"\n",
    "    if not lyrics or lyrics.strip() == '':\n",
    "        # Return zeros if lyrics are missing, though this should be rare for the 76 rows we target, we cleaned before.\n",
    "        return {\n",
    "            'n_sentences': 0, \n",
    "            'n_tokens': 0, \n",
    "            'char_per_tok': 0.0, \n",
    "            'avg_token_per_clause': 0.0\n",
    "        }\n",
    "    # 1. Tokenization (Counting Words/Tokens) --> to fill 'n_tokens'\n",
    "    # Use a regex to split the text by any sequence of non-alphanumeric characters (including spaces, punctuation, etc.)\n",
    "    # and filter out empty strings resulting from the split.\n",
    "    words = [token for token in re.split(r'[^a-zA-Z0-9]+', lyrics) if token]\n",
    "    n_tokens = len(words)\n",
    "\n",
    "    # 2. Character Count\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "\n",
    "    # 3. Sentence Count\n",
    "    # Assuming sentences are separated by newline characters (the simplest heuristic)\n",
    "    # This might overestimate or underestimate the true sentence count.\n",
    "    sentences = [s for s in lyrics.split('\\n') if s.strip() != '']\n",
    "    n_sentences = len(sentences)\n",
    "    \n",
    "    # 4. Calculate Derived Metrics\n",
    "    \n",
    "    # Average characters per token\n",
    "    char_per_tok = total_chars / n_tokens if n_tokens > 0 else 0.0\n",
    "    \n",
    "    # Average tokens per clause (using sentences as clauses)\n",
    "    avg_token_per_clause = n_tokens / n_sentences if n_sentences > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'n_sentences': n_sentences, \n",
    "        'n_tokens': n_tokens, \n",
    "        'char_per_tok': round(char_per_tok, 4), # Rounding for clean output\n",
    "        'avg_token_per_clause': round(avg_token_per_clause, 4)\n",
    "    }\n",
    "\n",
    "# --- Main Processing ---\n",
    "\n",
    "def fill_missing_metrics(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Main function to read the CSV, fill the missing metrics using lyrics, and write the new CSV.\n",
    "    \"\"\"\n",
    "    updated_rows_count = 0\n",
    "    \n",
    "    try:\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames # Get the original header\n",
    "            \n",
    "            # Read all rows into memory for easier manipulation (assuming the file is not excessively large)\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found at {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Process Data\n",
    "    for row in data:\n",
    "        # Check if the row is one of the 76 missing ones (assuming missing values were left as empty strings)\n",
    "        # We also check if 'lyrics' is NOT missing, otherwise we cannot calculate\n",
    "        missing_values = ['', 'NaN', 'nan', None]\n",
    "        is_metrics_missing = any(row.get(col) in missing_values for col in columnsToFill)\n",
    "        has_lyrics = row.get('lyrics', '').strip() not in ['', 'NaN', 'nan']\n",
    "        \n",
    "        if is_metrics_missing and has_lyrics:\n",
    "            \n",
    "            # Calculate the new metrics using the available lyrics\n",
    "            new_metrics = calculate_metrics(row['lyrics'])\n",
    "            \n",
    "            # Update the row with the calculated values\n",
    "            for col, value in new_metrics.items():\n",
    "                row[col] = str(value) # CSV writers expect string values\n",
    "            \n",
    "            updated_rows_count += 1\n",
    "            \n",
    "        elif is_metrics_missing and not has_lyrics:\n",
    "             # In a production setting, you'd log or handle this case. \n",
    "             # Since we only removed 3 'lyrics' rows, this shouldn't affect the other 76 rows.\n",
    "             pass \n",
    "\n",
    "    # Write the updated data to the new file\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\" Data imputation completed.\")\n",
    "    print(f\" Total rows updated with calculated metrics: {updated_rows_count}\")\n",
    "    print(f\" Cleaned and imputed data saved to: {output_file}\")\n",
    "\n",
    "# Execute the main function\n",
    "fill_missing_metrics(input_file1, output_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb19d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Spotify API Imputation ---\n",
      "\n",
      " Spotify Imputation Complete.\n",
      " Total rows successfully imputed: 71\n",
      " Data saved to: DataPostSpotifyAPI.csv\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import base64\n",
    "import time\n",
    "\n",
    "# --- Configuration and Constants ---\n",
    "\n",
    "INPUT_FILE = 'audioCleanData.csv' \n",
    "OUTPUT_FILE = 'DataPostSpotifyAPI.csv'\n",
    "MISSING_VALUES = ['', 'NaN', 'nan'] \n",
    "\n",
    "# !!! REPLACE THESE WITH YOUR ACTUAL SPOTIFY CREDENTIALS !!!\n",
    "SPOTIFY_CLIENT_ID = \"5838810d86504d6bbc6947b39b82d8f7\" \n",
    "SPOTIFY_CLIENT_SECRET = \"b467db2aef154993a7610f779203bc87\" \n",
    "\n",
    "# Spotify API Endpoints\n",
    "SPOTIFY_TOKEN_URL = \"https://accounts.spotify.com/api/token\"\n",
    "SPOTIFY_SEARCH_URL = \"https://api.spotify.com/v1/search\"\n",
    "SPOTIFY_FEATURES_URL = \"https://api.spotify.com/v1/audio-features\"\n",
    "\n",
    "# Columns to be Imputed\n",
    "AUDIO_COLUMNS = ['bpm', 'loudness', 'pitch', 'flux', 'rms', 'rolloff', 'flatness', 'spectral_complexity']\n",
    "ALBUM_COLUMNS = ['album_name', 'album_release_date', 'album_type', 'disc_number', 'track_number', 'duration_ms', 'explicit', 'popularity', 'id_album']\n",
    "LANGUAGE_COLUMN = 'language' \n",
    "ALL_EXTERNAL_COLUMNS = AUDIO_COLUMNS + ALBUM_COLUMNS + [LANGUAGE_COLUMN]\n",
    "\n",
    "# --- 1. Spotify Authentication ---\n",
    "\n",
    "def get_spotify_token(client_id: str, client_secret: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Obtains an Access Token from Spotify using the Client Credentials Flow.\n",
    "    This token is required for all subsequent API requests.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Base64 encode the Client ID and Secret for authentication header\n",
    "        auth_string = f\"{client_id}:{client_secret}\"\n",
    "        encoded_auth = base64.b64encode(auth_string.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Basic {encoded_auth}\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "        }\n",
    "        data = urllib.parse.urlencode({'grant_type': 'client_credentials'}).encode('utf-8')\n",
    "\n",
    "        req = urllib.request.Request(SPOTIFY_TOKEN_URL, data=data, headers=headers, method='POST')\n",
    "        \n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            token_info = json.loads(response.read().decode('utf-8'))\n",
    "            return token_info.get('access_token')\n",
    "            \n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"Error getting Spotify token: HTTP {e.code} - {e.reason}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Spotify token: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Spotify Data Fetching ---\n",
    "\n",
    "def fetch_spotify_data(title: str, artist: str, token: str) -> dict:\n",
    "    \"\"\"\n",
    "    Searches Spotify for the track ID and then fetches its metadata and audio features.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing track metadata and audio features, or {} on failure.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    track_id = None\n",
    "    \n",
    "    # --- PHASE A: Search for Track ID ---\n",
    "    \n",
    "    try:\n",
    "        search_query = f\"track:{title} artist:{artist}\"\n",
    "        encoded_query = urllib.parse.quote(search_query)\n",
    "        search_url = f\"{SPOTIFY_SEARCH_URL}?q={encoded_query}&type=track&limit=1\"\n",
    "        \n",
    "        req = urllib.request.Request(search_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            search_results = json.loads(response.read().decode('utf-8'))\n",
    "        \n",
    "        items = search_results.get('tracks', {}).get('items', [])\n",
    "        \n",
    "        if not items:\n",
    "            return {} # Track not found\n",
    "        \n",
    "        # Extract ID and initial metadata\n",
    "        track_info = items[0]\n",
    "        track_id = track_info['id']\n",
    "        album_info = track_info.get('album', {})\n",
    "        \n",
    "        # Prepare initial data dictionary\n",
    "        data = {\n",
    "            'duration_ms': track_info.get('duration_ms'),\n",
    "            'explicit': str(track_info.get('explicit')),\n",
    "            'popularity': track_info.get('popularity'),\n",
    "            'album_name': album_info.get('name'),\n",
    "            'album_release_date': album_info.get('release_date'),\n",
    "            'album_type': album_info.get('album_type'),\n",
    "            'disc_number': track_info.get('disc_number'),\n",
    "            'track_number': track_info.get('track_number'),\n",
    "            'id_album': album_info.get('id'),\n",
    "            # Language is not reliably available via search endpoint, handle later\n",
    "        }\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Rate limit or other HTTP error\n",
    "        print(f\"DEBUG: Search failed for '{title}': HTTP {e.code}\")\n",
    "        if e.code == 429: # Too Many Requests\n",
    "            print(\"WARNING: Spotify Rate Limit hit. Pausing for 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        return {}\n",
    "    except Exception:\n",
    "        return {} # Parsing or network error\n",
    "\n",
    "    # --- PHASE B: Fetch Audio Features using Track ID ---\n",
    "\n",
    "    try:\n",
    "        features_url = f\"{SPOTIFY_FEATURES_URL}/{track_id}\"\n",
    "        req = urllib.request.Request(features_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            audio_features = json.loads(response.read().decode('utf-8'))\n",
    "        \n",
    "        # Impute audio features, mapping Spotify names to CSV column names\n",
    "        data['bpm'] = audio_features.get('tempo')\n",
    "        data['loudness'] = audio_features.get('loudness')\n",
    "        data['pitch'] = audio_features.get('key') # 'key' represents pitch class (0-11)\n",
    "        # Note: Spotify does not provide 'flux', 'rms', 'flatness', 'spectral_complexity', \n",
    "        # or 'rolloff' directly. These are usually derived from raw audio analysis.\n",
    "        # We will leave these as missing if Spotify doesn't have a direct equivalent.\n",
    "        \n",
    "        # --- Handle non-available columns (as required) ---\n",
    "        # If Spotify cannot provide these, they remain missing (NaN/empty string).\n",
    "        \n",
    "    except Exception:\n",
    "        # Audio feature fetching failed (e.g., track is missing features)\n",
    "        pass \n",
    "        \n",
    "    return data # Returns the gathered data (potentially incomplete)\n",
    "\n",
    "# --- 3. Main Imputation Logic ---\n",
    "\n",
    "def impute_external_data_spotify(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Implements the conservative imputation using the Spotify API.\n",
    "    Only fills the fields if data is successfully fetched; otherwise, preserves original value.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Spotify API Imputation ---\")\n",
    "    \n",
    "    # 1. Get Spotify Access Token\n",
    "    spotify_token = get_spotify_token(SPOTIFY_CLIENT_ID, SPOTIFY_CLIENT_SECRET)\n",
    "    if not spotify_token:\n",
    "        print(\"FATAL: Could not retrieve Spotify token. Cannot proceed with API calls.\")\n",
    "        return\n",
    "\n",
    "    imputed_count = 0\n",
    "    data_to_write = []\n",
    "\n",
    "    try:\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader) \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input file not found at {input_file}.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate and Impute\n",
    "    for i, row in enumerate(data):\n",
    "        # Check if ANY external column is missing\n",
    "        is_missing_external = any(row.get(col, '').strip().lower() in MISSING_VALUES for col in ALL_EXTERNAL_COLUMNS)\n",
    "        \n",
    "        if is_missing_external and row.get('title') and row.get('primary_artist'):\n",
    "            \n",
    "            # Fetch data (will be {} if not found)\n",
    "            external_data = fetch_spotify_data(row['title'], row['primary_artist'], spotify_token)\n",
    "            updated = False\n",
    "            \n",
    "            # Impute the missing fields ONLY IF external_data has a specific value\n",
    "            for col in ALL_EXTERNAL_COLUMNS:\n",
    "                current_value = str(row.get(col, '')).strip().lower()\n",
    "                \n",
    "                # Check 1: Is the current value missing? \n",
    "                # Check 2: Did Spotify successfully return a non-None value for this column?\n",
    "                if current_value in MISSING_VALUES and col in external_data and external_data[col] is not None:\n",
    "                    row[col] = str(external_data[col]) # Update the row\n",
    "                    updated = True\n",
    "            \n",
    "            if updated:\n",
    "                imputed_count += 1\n",
    "        \n",
    "        data_to_write.append(row)\n",
    "        \n",
    "        # Add a delay to respect Spotify's rate limits\n",
    "        if i % 10 == 0 and i > 0:\n",
    "             time.sleep(0.1) \n",
    "\n",
    "    # 3. Write the updated data to the new file\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Spotify Imputation Complete.\")\n",
    "    print(f\" Total rows successfully imputed: {imputed_count}\")\n",
    "    print(f\" Data saved to: {output_file}\")\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "impute_external_data_spotify(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3680722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Date part imputation complete.\n",
      " Total rows where at least one date part was imputed: 1235\n",
      " File saved to: DataWithDateImputation.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'DataPostSpotifyAPI.csv' \n",
    "OUTPUT_FILE = 'DataWithDateImputation.csv'\n",
    "\n",
    "# Define all possible representations of missing data\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', None] \n",
    "\n",
    "# --- Main Imputation Function ---\n",
    "\n",
    "def impute_date_parts(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Reads the CSV and imputes missing values in existing 'year', 'month', \n",
    "    and 'day' columns using data from 'album_release_date'.\n",
    "    \"\"\"\n",
    "    data_to_write = []\n",
    "    imputed_rows = 0\n",
    "    \n",
    "    try:\n",
    "        # Read the input file and determine the fieldnames\n",
    "        with open(input_file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "            reader = csv.DictReader(infile)\n",
    "            fieldnames = reader.fieldnames\n",
    "            data = list(reader)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\" ERROR: Input file not found at: {input_file}\")\n",
    "        return\n",
    "\n",
    "    # Ensure the target columns exist in the file\n",
    "    if not all(col in fieldnames for col in ['year', 'month', 'day']):\n",
    "        print(\" ERROR: Required columns ('year', 'month', 'day') not found in the input file.\")\n",
    "        return\n",
    "        \n",
    "    for row in data:\n",
    "        release_date = row.get('album_release_date')\n",
    "        \n",
    "        # Check if the source date is available and valid\n",
    "        if release_date is None or str(release_date).strip().lower() in MISSING_VALUES:\n",
    "            # If the source date is missing, we cannot impute, so we skip\n",
    "            data_to_write.append(row)\n",
    "            continue\n",
    "            \n",
    "        date_updated = False\n",
    "        \n",
    "        # Determine which columns are currently missing\n",
    "        is_year_missing = str(row.get('year', '')).strip().lower() in MISSING_VALUES\n",
    "        is_month_missing = str(row.get('month', '')).strip().lower() in MISSING_VALUES\n",
    "        is_day_missing = str(row.get('day', '')).strip().lower() in MISSING_VALUES\n",
    "        \n",
    "        # Only proceed if at least one target column is missing\n",
    "        if is_year_missing or is_month_missing or is_day_missing:\n",
    "            \n",
    "            # --- Parsing Logic ---\n",
    "            try:\n",
    "                # 1. Try YYYY-MM-DD format (Full date)\n",
    "                dt_obj = datetime.strptime(release_date, '%Y-%m-%d')\n",
    "                \n",
    "                if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                if is_month_missing: row['month'] = dt_obj.month; date_updated = True\n",
    "                if is_day_missing: row['day'] = dt_obj.day; date_updated = True\n",
    "                \n",
    "            except ValueError:\n",
    "                # 2. Try YYYY-MM format (Month granularity)\n",
    "                try:\n",
    "                    dt_obj = datetime.strptime(release_date, '%Y-%m')\n",
    "                    \n",
    "                    if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                    if is_month_missing: row['month'] = dt_obj.month; date_updated = True\n",
    "                    # Day remains missing if it was missing initially\n",
    "                    \n",
    "                except ValueError:\n",
    "                    # 3. Try YYYY format (Year granularity only)\n",
    "                    try:\n",
    "                        dt_obj = datetime.strptime(release_date, '%Y')\n",
    "                        \n",
    "                        if is_year_missing: row['year'] = dt_obj.year; date_updated = True\n",
    "                        # Month and Day remain missing if they were missing initially\n",
    "                        \n",
    "                    except ValueError:\n",
    "                        # Parsing failed completely or the date is malformed\n",
    "                        pass \n",
    "        \n",
    "        if date_updated:\n",
    "            imputed_rows += 1\n",
    "            \n",
    "        data_to_write.append(row)\n",
    "\n",
    "    # --- Write Output ---\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data_to_write)\n",
    "\n",
    "    print(f\"\\n Date part imputation complete.\")\n",
    "    print(f\" Total rows where at least one date part was imputed: {imputed_rows}\")\n",
    "    print(f\" File saved to: {output_file}\")\n",
    "\n",
    "# Execute the function\n",
    "impute_date_parts(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d075ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST TO CHECK--> CANNOT USE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# dfffff = pd.read_csv('DataWithDateImputation.csv')\n",
    "\n",
    "# missing_countsPostSpotify = dfffff.isnull().sum() \n",
    "# # print(\"The number of missing values is: \\n\",missing_counts[missing_counts > 0]) #28 columns out of 37\n",
    "# missing_countsPostSpotify = missing_countsPostSpotify[missing_countsPostSpotify > 0].to_frame(name=\"missing_values\")\n",
    "# print(missing_countsPostSpotify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dab797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV_FILE = 'output2.csv'\n",
    "OUTPUT_FILE = 'artistClean.csv'\n",
    "USER_AGENT = \"MyApp/1.0 (email@example.com)\" # Required by APIs for identification\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', '0', 0] # Define all values that count as missing/zero\n",
    "\n",
    "# --- MusicBrainz Endpoints ---\n",
    "MB_URL = \"http://musicbrainz.org/ws/2/artist/\"\n",
    "NOMINATIM_URL = \"http://nominatim.openstreetmap.org/search\" # OpenStreetMap Geocoding\n",
    "\n",
    "# --- Utility Functions (APIs using urllib) ---\n",
    "\n",
    "def safe_http_get(url: str, params: dict, headers: dict) -> dict | None:\n",
    "    \"\"\"Performs a GET request using urllib and returns JSON data.\"\"\"\n",
    "    \n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    full_url = f\"{url}?{query_string}\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() == 200:\n",
    "                data = response.read().decode('utf-8')\n",
    "                return json.loads(data)\n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.getcode()} for URL: {url}\")\n",
    "                return None\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Catch specific HTTP errors\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch network or parsing errors\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    data = safe_http_get(MB_URL, params, headers)\n",
    "    time.sleep(1) # Respect API rate limits (1 request/second)\n",
    "    \n",
    "    if data is None or \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        # MusicBrainz usually provides the start date of the lifespan\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"), \n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # Extract country from the 'area' object\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "        \n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract country (Simplified)\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country using heuristics.\n",
    "    Returns a dictionary with available country data.\n",
    "    \"\"\"\n",
    "    wiki_url = \"https://en.wikipedia.org/api/rest_v1/page/summary/\"\n",
    "    search_path = urllib.parse.quote(name.replace(' ', '_'))\n",
    "    full_url = f\"{wiki_url}{search_path}\"\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    # Since Wikipedia API endpoint does not take params in the same way, we request directly\n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() != 200: return None\n",
    "            data = json.loads(response.read().decode('utf-8'))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(1) # Respect rate limits\n",
    "    \n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description (as per original logic)\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place (OpenStreetMap Nominatim)\n",
    "# ----------------------------\n",
    "def geocode_place(place_name: str, country_name: str | None = None) -> tuple[float or None, float or None]:\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "        \n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    # Nominatim requires a distinct User-Agent and is sensitive to rate limits\n",
    "    headers = {\"User-Agent\": \"MyDataPipeline/1.0 (contact@example.com)\"} \n",
    "\n",
    "    # Geocoding logic using safe_http_get structure\n",
    "    data = safe_http_get(NOMINATIM_URL, params, headers)\n",
    "    time.sleep(1) # Respect Nominatim usage policy (1 request/second)\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "    except (ValueError, KeyError):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_CSV_FILE = 'output2.csv'\n",
    "OUTPUT_FILE = 'artistClean.csv'\n",
    "USER_AGENT = \"MyApp/1.0 (email@example.com)\" # Required by APIs for identification\n",
    "MISSING_VALUES = ['', 'NaN', 'nan', '0', 0] # Define all values that count as missing/zero\n",
    "\n",
    "# --- MusicBrainz Endpoints ---\n",
    "MB_URL = \"http://musicbrainz.org/ws/2/artist/\"\n",
    "NOMINATIM_URL = \"http://nominatim.openstreetmap.org/search\" # OpenStreetMap Geocoding\n",
    "\n",
    "# --- Utility Functions (APIs using urllib) ---\n",
    "\n",
    "def safe_http_get(url: str, params: dict, headers: dict) -> dict | None:\n",
    "    \"\"\"Performs a GET request using urllib and returns JSON data.\"\"\"\n",
    "    \n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    full_url = f\"{url}?{query_string}\"\n",
    "    \n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() == 200:\n",
    "                data = response.read().decode('utf-8')\n",
    "                return json.loads(data)\n",
    "            else:\n",
    "                print(f\"HTTP Error: {response.getcode()} for URL: {url}\")\n",
    "                return None\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Catch specific HTTP errors\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch network or parsing errors\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    data = safe_http_get(MB_URL, params, headers)\n",
    "    time.sleep(1) # Respect API rate limits (1 request/second)\n",
    "    \n",
    "    if data is None or \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        # MusicBrainz usually provides the start date of the lifespan\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"), \n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # Extract country from the 'area' object\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "        \n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract country (Simplified)\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country using heuristics.\n",
    "    Returns a dictionary with available country data.\n",
    "    \"\"\"\n",
    "    wiki_url = \"https://en.wikipedia.org/api/rest_v1/page/summary/\"\n",
    "    search_path = urllib.parse.quote(name.replace(' ', '_'))\n",
    "    full_url = f\"{wiki_url}{search_path}\"\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    \n",
    "    # Since Wikipedia API endpoint does not take params in the same way, we request directly\n",
    "    try:\n",
    "        req = urllib.request.Request(full_url, headers=headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            if response.getcode() != 200: return None\n",
    "            data = json.loads(response.read().decode('utf-8'))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    time.sleep(1) # Respect rate limits\n",
    "    \n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description (as per original logic)\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place (OpenStreetMap Nominatim)\n",
    "# ----------------------------\n",
    "def geocode_place(place_name: str, country_name: str | None = None) -> tuple[float or None, float or None]:\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "        \n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    # Nominatim requires a distinct User-Agent and is sensitive to rate limits\n",
    "    headers = {\"User-Agent\": \"MyDataPipeline/1.0 (contact@example.com)\"} \n",
    "\n",
    "    # Geocoding logic using safe_http_get structure\n",
    "    data = safe_http_get(NOMINATIM_URL, params, headers)\n",
    "    time.sleep(1) # Respect Nominatim usage policy (1 request/second)\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        return None, None\n",
    "    try:\n",
    "        return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "    except (ValueError, KeyError):\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7451e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search an artist on MusicBrainz\n",
    "# ----------------------------\n",
    "def search_artist_musicbrainz(name):\n",
    "    \"\"\"\n",
    "    Search for an artist on MusicBrainz using their name.\n",
    "    Returns a dictionary with available data: birth_place, country, birth_date, active_start, active_end\n",
    "    \"\"\"\n",
    "    url = \"https://musicbrainz.org/ws/2/artist/\"\n",
    "    params = {\n",
    "        \"query\": name,\n",
    "        \"fmt\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"MyApp/1.0 (email@example.com)\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    data = r.json()\n",
    "    if \"artists\" not in data or len(data[\"artists\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    artist_info = data[\"artists\"][0]\n",
    "    result = {\n",
    "        \"birth_place\": None,\n",
    "        \"country\": None,\n",
    "        \"birth_date\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_start\": artist_info.get(\"life-span\", {}).get(\"begin\"),\n",
    "        \"active_end\": artist_info.get(\"life-span\", {}).get(\"end\")\n",
    "    }\n",
    "    # MusicBrainz provides the artist's area (country)\n",
    "    if \"area\" in artist_info:\n",
    "        result[\"country\"] = artist_info[\"area\"].get(\"name\")\n",
    "    return result\n",
    "\n",
    "# ----------------------------\n",
    "# Function to search Wikipedia for artist and extract city/country\n",
    "# ----------------------------\n",
    "def search_artist_wikipedia(name):\n",
    "    \"\"\"\n",
    "    Search the artist's Wikipedia page and try to extract the country.\n",
    "    Returns a dictionary with available data.\n",
    "    \"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{name.replace(' ', '_')}\"\n",
    "    headers = {\"User-Agent\": \"MyApp/1.0 (email@example.com)\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    data = r.json()\n",
    "    if \"description\" in data:\n",
    "        desc = data[\"description\"].lower()\n",
    "        country = None\n",
    "        # Basic heuristics based on nationality in description\n",
    "        if \"italian\" in desc:\n",
    "            country = \"Italy\"\n",
    "        elif \"american\" in desc:\n",
    "            country = \"USA\"\n",
    "        # add more cases if needed\n",
    "        return {\"country\": country}\n",
    "    return None\n",
    "\n",
    "# ----------------------------\n",
    "# Function for geocoding a place\n",
    "# ----------------------------\n",
    "def geocode_place(place_name, country_name=None):\n",
    "    \"\"\"\n",
    "    Get latitude and longitude of a place using OpenStreetMap Nominatim.\n",
    "    \"\"\"\n",
    "    query = place_name\n",
    "    if country_name:\n",
    "        query += f\", {country_name}\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": query, \"format\": \"json\", \"limit\": 1}\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        r.raise_for_status()\n",
    "    except:\n",
    "        return None, None\n",
    "    \n",
    "    data = r.json()\n",
    "    if len(data) == 0:\n",
    "        return None, None\n",
    "    return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "\n",
    "# ----------------------------\n",
    "# Update artists without latitude/longitude\n",
    "# ----------------------------\n",
    "for artist in df_artists:\n",
    "    if artist[\"latitude\"] == 0 or artist[\"longitude\"] == 0:\n",
    "        print(f\"Processing: {artist['name']}\")\n",
    "        \n",
    "        # 1️⃣ Try MusicBrainz first\n",
    "        info = search_artist_musicbrainz(artist[\"name\"])\n",
    "        time.sleep(1)\n",
    "        if info:\n",
    "            artist[\"birth_place\"] = info.get(\"birth_place\") or artist.get(\"birth_place\")\n",
    "            artist[\"country\"] = info.get(\"country\") or artist.get(\"country\")\n",
    "            artist[\"birth_date\"] = info.get(\"birth_date\") or artist.get(\"birth_date\")\n",
    "            artist[\"active_start\"] = info.get(\"active_start\") or artist.get(\"active_start\")\n",
    "            artist[\"active_end\"] = info.get(\"active_end\") or artist.get(\"active_end\")\n",
    "        \n",
    "        # 2️⃣ If country still missing, try Wikipedia\n",
    "        if not artist.get(\"country\"):\n",
    "            wiki_info = search_artist_wikipedia(artist[\"name\"])\n",
    "            time.sleep(1)\n",
    "            if wiki_info and wiki_info.get(\"country\"):\n",
    "                artist[\"country\"] = wiki_info[\"country\"]\n",
    "        \n",
    "        # 3️⃣ Geocode if we have at least birth_place or country\n",
    "        place = artist.get(\"birth_place\") or artist.get(\"country\")\n",
    "        country = artist.get(\"country\") if artist.get(\"birth_place\") else None\n",
    "        if place:\n",
    "            lat, lon = geocode_place(place, country)\n",
    "            if lat is not None and lon is not None:\n",
    "                artist[\"latitude\"] = lat\n",
    "                artist[\"longitude\"] = lon\n",
    "            time.sleep(1)\n",
    "\n",
    "# ----------------------------\n",
    "# Final check of artists data\n",
    "# ----------------------------\n",
    "for artist in df_artists:\n",
    "    print(artist[\"name\"], artist[\"birth_place\"], artist[\"country\"], artist[\"latitude\"], artist[\"longitude\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
